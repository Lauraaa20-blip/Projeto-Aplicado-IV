# -*- coding: utf-8 -*-
"""PROJAPLICIV FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W5RpKKCSe8cf51FrCzkq35fIxjcntQLs

**DEPENDÊNCIAS**
"""

!pip install pandas numpy matplotlib seaborn scikit-learn pmdarima prophet tensorflow==2.19.0 --quiet

"""**Etapa 1 — Upload e Carregamento dos CSVs**"""

import os
import shutil
from glob import glob
from google.colab import files
import pandas as pd

# Criar pasta data
DATA_DIR = '/content/data'
os.makedirs(DATA_DIR, exist_ok=True)

# Upload dos arquivos CSV
uploaded = files.upload()

# Mover arquivos para /content/data
for filename in uploaded.keys():
    shutil.move(filename, os.path.join(DATA_DIR, filename))

# Conferir arquivos
print("Arquivos disponíveis:")
!ls /content/data

# Função de leitura segura e concatenação
def load_infodengue_files_safe(file_list):
    dfs = []
    for f in file_list:
        try:
            try:
                df = pd.read_csv(f, low_memory=False)
            except:
                df = pd.read_csv(f, low_memory=False, encoding='latin1', sep=';')
            if df.empty:
                print(f"Aviso: arquivo vazio {f}")
                continue
            # Extrair capital do nome do arquivo
            capital_name = os.path.basename(f).replace('infodengue_', '').replace('.csv', '')
            df['capital'] = capital_name
            df['source_file'] = os.path.basename(f)
            dfs.append(df)
        except Exception as e:
            print(f"Erro ao ler {f}: {e}")
    if dfs:
        return pd.concat(dfs, ignore_index=True)
    else:
        print("Nenhum CSV válido encontrado.")
        return pd.DataFrame()

csv_files = sorted(glob(os.path.join(DATA_DIR, "*.csv")))
df_raw = load_infodengue_files_safe(csv_files)
print("Registros carregados:", df_raw.shape)
df_raw.head()

import os
import pandas as pd
from glob import glob

DATA_DIR = "/content/data"
csvs = sorted(glob(os.path.join(DATA_DIR, "infodengue_*.csv")))

dfs = []
for f in csvs:
    df_tmp = pd.read_csv(f, low_memory=False)
    # Criar coluna 'capital' a partir do nome do arquivo
    capital_name = os.path.basename(f).replace('infodengue_','').replace('.csv','').lower()
    df_tmp['capital'] = capital_name
    dfs.append(df_tmp)

df_raw = pd.concat(dfs, ignore_index=True)
print(df_raw[['capital','data_iniSE','casos']].head())

"""**Etapa 2 — Ajuste de Colunas**"""

rename_map = {
    'sem_epi': 'week_epi',
    'data': 'date',
    'casos': 'cases',
    'municipio': 'municipality',
    'uf': 'state',
    'temp_media': 'temp_mean',
    'umidade': 'humidity',
    'precipitacao': 'precipitation'
}

df = df_raw.copy()
df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns}, inplace=True)

# Garantir coluna de data a partir de year + week_epi
if 'date' not in df.columns and 'year' in df.columns and 'week_epi' in df.columns:
    df['week_epi'] = df['week_epi'].astype(int)
    df['date'] = pd.to_datetime(
        df['year'].astype(str) + '-W' + df['week_epi'].astype(str) + '-1',
        format='%Y-W%W-%w'
    )

# Garantir coluna 'cases' numérica
if 'cases' in df.columns:
    df['cases'] = pd.to_numeric(df['cases'], errors='coerce').fillna(0).astype(int)

df.info()

import os
from glob import glob
import pandas as pd

DATA_DIR = '/content/data'
csv_files = sorted(glob(os.path.join(DATA_DIR, "*.csv")))

for f in csv_files:
    print(f"\n--- Arquivo: {os.path.basename(f)} ---")
    df_tmp = pd.read_csv(f, nrows=5, low_memory=False)
    print(df_tmp.head())
    print("Colunas:", df_tmp.columns.tolist())

# Criar cópia
df = df_raw.copy()

# Renomear colunas principais
rename_map = {
    'data_iniSE': 'date',
    'casos': 'cases',
    'tempmed': 'temp_mean',
    'umidmed': 'humidity',
    'precipitacao': 'precipitation'  # se não existir, será ignorado
}
df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns}, inplace=True)

# Converter coluna de data
df['date'] = pd.to_datetime(df['date'], errors='coerce')

# Garantir 'cases' numérico
df['cases'] = pd.to_numeric(df['cases'], errors='coerce').fillna(0).astype(int)

# Visualizar colunas e tipos
df.info()

"""**Etapa 3 - Filtrar Capitais e Agregar Semanalmente**"""

import matplotlib.pyplot as plt

# Selecionar apenas as capitais desejadas (todas que temos)
capitals = ['saopaulo', 'rio de janeiro', 'minas gerais', 'salvador', 'manaus', 'curitiba', 'goiania']
# Ajuste conforme nomes reais no CSV, por exemplo 'saopaulo' etc.
# Pelo que você mostrou, temos: 'curitiba', 'goiania', 'manaus', 'salvador', 'saopaulo'
capitals = ['curitiba', 'goiania', 'manaus', 'salvador', 'saopaulo']

df_sel = df[df['capital'].isin(capitals)].copy()

# Agregar casos por capital e semana
df_weekly = df_sel.groupby(['capital', 'date']).agg({
    'cases': 'sum',
    'temp_mean': 'mean' if 'temp_mean' in df_sel.columns else 'first',
    'humidity': 'mean' if 'humidity' in df_sel.columns else 'first'
}).reset_index()

# Visualizar exemplo: casos semanais de cada capital
for city in capitals:
    df_city = df_weekly[df_weekly['capital'] == city].sort_values('date')
    plt.plot(df_city['date'], df_city['cases'], marker='o', label=city)

plt.title('Casos semanais por capital')
plt.xlabel('Data')
plt.ylabel('Casos')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Etapa 4 — Série temporal agregada**"""

# Agregar casos das capitais para representar "mini-nacional"
df_national = df_weekly.groupby('date').agg({
    'cases': 'sum',
    'temp_mean': 'mean' if 'temp_mean' in df_weekly.columns else 'first',
    'humidity': 'mean' if 'humidity' in df_weekly.columns else 'first'
}).reset_index().sort_values('date')

df_national.set_index('date', inplace=True)

# (e a célula que plota o gráfico também, para garantir)
plt.figure(figsize=(10,5))
plt.plot(df_national.index, df_national['cases'], marker='o')
plt.title('Casos semanais agregados (todas as capitais)')
plt.show()

"""**Etapa 5 — Análise exploratória rápida - EDA**"""

import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose

# Estatísticas descritivas
df_national['cases'].describe()

# Decomposição sazonal (ex.: modelo aditivo, 52 semanas de periodicidade)
decomp = seasonal_decompose(df_national['cases'], model='additive', period=52)
decomp.plot()
plt.show()

# Correlação entre casos e variáveis climáticas, se disponíveis
vars_to_corr = ['cases', 'temp_mean', 'humidity']
sns.pairplot(df_national[vars_to_corr].dropna())
plt.show()

"""**Etapa 6 - Preparação dos Dados para Modelagem**"""

# --- ETAPA 6: PREPARAÇÃO DOS DADOS ---
import numpy as np

# Usaremos o df_national que você já criou
df_model = df_national.copy()

# 1. Feature Engineering
# Features de tempo
df_model['week_of_year'] = df_model.index.isocalendar().week
df_model['month'] = df_model.index.month
df_model['year'] = df_model.index.year

# Features de Lag (ex: usando as últimas 4 semanas para prever a atual)
for lag in range(1, 5):
    df_model[f'cases_lag_{lag}'] = df_model['cases'].shift(lag)

# Features de Média Móvel
df_model['rolling_mean_4'] = df_model['cases'].shift(1).rolling(window=4).mean()

# Remover linhas com NaN geradas pelos lags
df_model.dropna(inplace=True)

# 2. Divisão em Treino e Teste (dados até o final de 2023 para treino, 2024 em diante para teste)
split_date = '2024-01-01'
df_train = df_model[df_model.index < split_date]
df_test = df_model[df_model.index >= split_date]

# Separar features (X) e alvo (y)
features = ['week_of_year', 'month', 'year', 'cases_lag_1', 'cases_lag_2', 'cases_lag_3', 'cases_lag_4', 'rolling_mean_4', 'temp_mean', 'humidity']
target = 'cases'

X_train = df_train[features]
y_train = df_train[target]
X_test = df_test[features]
y_test = df_test[target]

print("Tamanho do Treino:", X_train.shape)
print("Tamanho do Teste:", X_test.shape)

"""**Etapa 7: Testando Modelos**

**Modelo 1: Prophet**
"""

# --- MODELO 1 (Alternativo): PROPHET ---
from prophet import Prophet
import pandas as pd
import matplotlib.pyplot as plt

# 1. Preparar os dados para o formato do Prophet
# O Prophet exige que as colunas se chamem 'ds' (para a data) e 'y' (para o valor alvo).
# Também vamos manter as colunas de temperatura e umidade, que usaremos como 'regressores'.
df_train_prophet = df_train.reset_index().rename(columns={'date': 'ds', 'cases': 'y'})
df_train_prophet = df_train_prophet[['ds', 'y', 'temp_mean', 'humidity']]

# 2. Criar e treinar o modelo
# Instanciamos o Prophet e adicionamos os regressores (features extras).
model_prophet = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
model_prophet.add_regressor('temp_mean')
model_prophet.add_regressor('humidity')

# Treinamos o modelo com os dados de treino
model_prophet.fit(df_train_prophet)

# 3. Preparar o DataFrame "futuro" para fazer as previsões
# Para prever, precisamos de um DataFrame com as datas futuras e os valores dos regressores nessas datas.
# Usaremos os dados do nosso conjunto de teste para isso.
future_df = df_test.reset_index()[['date', 'temp_mean', 'humidity']].rename(columns={'date': 'ds'})

# 4. Fazer as previsões
forecast = model_prophet.predict(future_df)

# A previsão fica na coluna 'yhat'. Vamos guardá-la em uma Series para facilitar a comparação.
predictions_prophet = forecast['yhat']
predictions_prophet.index = y_test.index # Ajusta o índice para ser igual ao do y_test

# 5. Visualizar os resultados
plt.figure(figsize=(12, 6))
plt.plot(y_train.tail(200), label='Treino (últimas 200 semanas)') # Plotar só o final do treino para melhor visualização
plt.plot(y_test, label='Real (Teste)', color='blue')
plt.plot(predictions_prophet, label='Previsão Prophet', color='purple', linestyle='--')
plt.title('Previsão com Prophet')
plt.legend()
plt.show()

# Um bônus do Prophet é que ele pode plotar os componentes da série (tendência, sazonalidade)
fig_components = model_prophet.plot_components(forecast)
plt.show()

"""**Modelo 2: XGBoost (Machine Learning)**"""

# --- MODELO 2: XGBOOST ---
import xgboost as xgb
from sklearn.metrics import mean_squared_error

xgb_reg = xgb.XGBRegressor(
    n_estimators=1000,
    learning_rate=0.01,
    objective='reg:squarederror',
    early_stopping_rounds=10,
    n_jobs=-1
)

# Treinar o modelo
xgb_reg.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=False
)

# Fazer previsões
predictions_xgb = xgb_reg.predict(X_test)
predictions_xgb = pd.Series(predictions_xgb, index=y_test.index)

# Plotar resultados
plt.figure(figsize=(12, 6))
plt.plot(y_train.loc['2020':], label='Treino') # Começa o treino visível em 2020
plt.plot(y_test, label='Real (Teste)')
plt.plot(predictions_xgb, label='Previsão XGBoost', color='green')
plt.title('Previsão com XGBoost')
plt.legend()
plt.show()

# Importância das features
pd.Series(xgb_reg.feature_importances_, index=features).sort_values().plot(kind='barh', title='Importância das Features - XGBoost')
plt.show()

"""**Etapa 8 - Avaliação e Comparação**"""

# --- ETAPA 8: AVALIAÇÃO E COMPARAÇÃO (ATUALIZADA) ---
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Criar um DataFrame para comparar os resultados
results = pd.DataFrame({
    'Real': y_test,
    'Prophet': predictions_prophet,
    'XGBoost': predictions_xgb # Gerado pelo código do Modelo 2
})

# Calcular métricas para o Prophet
mae_prophet = mean_absolute_error(y_test, predictions_prophet)
rmse_prophet = np.sqrt(mean_squared_error(y_test, predictions_prophet))

# Calcular métricas para o XGBoost (célula do modelo 2 já deve ter rodado)
mae_xgb = mean_absolute_error(y_test, predictions_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, predictions_xgb))


print("--- Resultados Prophet ---")
print(f"MAE: {mae_prophet:.2f}")
print(f"RMSE: {rmse_prophet:.2f}\n")

print("--- Resultados XGBoost ---")
print(f"MAE: {mae_xgb:.2f}")
print(f"RMSE: {rmse_xgb:.2f}\n")

# Plot final comparativo
results.plot(figsize=(14, 7))
plt.title('Comparação de Modelos vs. Dados Reais')
plt.ylabel('Casos de Dengue')
plt.legend()
plt.show()

"""**Etapa 9 - Ajuste Fino de Hiperparâmetros do XGBoost**"""

# --- ETAPA 9: AJUSTE DE HIPERPARÂMETROS (TUNING) ---
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
import xgboost as xgb

# Definir o modelo
xgb_tuner = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1)

# Definir o espaço de parâmetros para testar
# (Comece com poucos parâmetros para ser mais rápido)
param_grid = {
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 500, 1000],
    'learning_rate': [0.01, 0.1]
}

# Para validação cruzada em séries temporais, usamos o TimeSeriesSplit
# Ele garante que o treino sempre vem antes do teste
tscv = TimeSeriesSplit(n_splits=3)

# Configurar o Grid Search
grid_search = GridSearchCV(
    estimator=xgb_tuner,
    param_grid=param_grid,
    cv=tscv,
    scoring='neg_root_mean_squared_error', # Métrica para otimizar
    verbose=2
)

# Executar a busca pelos melhores parâmetros
grid_search.fit(X_train, y_train)

print("\nMelhores parâmetros encontrados:")
print(grid_search.best_params_)

# Guardar o melhor modelo encontrado
best_xgb_model = grid_search.best_estimator_

"""**Etapa 10 - Treinamento do Modelo Final e Previsão para o Futuro**"""

# --- ETAPA 10: MODELO FINAL E PREVISÃO FUTURA ---

# 1. Treinar o melhor modelo com TODOS os dados (treino + teste)
final_model = grid_search.best_estimator_
final_model.fit(
    pd.concat([X_train, X_test]),
    pd.concat([y_train, y_test])
)


# 2. Criar um DataFrame para as semanas futuras que queremos prever
# Ex: Prever as próximas 12 semanas
last_date = df_model.index.max()
future_dates = pd.date_range(start=last_date, periods=13, freq='W')[1:] # 13 períodos para ter 12 novas datas

future_df = pd.DataFrame(index=future_dates)
print(f"Datas a serem previstas:\n{future_df.index}")

# 3. Construir as features para essas datas futuras
# Isso é um desafio, pois não temos os 'casos' para criar os lags.
# A estratégia é prever uma semana de cada vez, usando a previsão anterior para criar o lag da próxima.

# Pegar a última linha dos nossos dados conhecidos
last_known_data = df_model.iloc[-1]

# Lista para guardar as previsões
future_predictions = []

current_features = last_known_data[features].copy()

for date in future_dates:
    # Prever usando as features atuais
    prediction = final_model.predict(pd.DataFrame([current_features]))[0]
    future_predictions.append(prediction)

    # Atualizar as features para a próxima iteração
    # Essa é a parte mais importante: a nova previsão se torna o lag_1 da próxima
    new_features = current_features.copy()
    new_features['cases_lag_4'] = new_features['cases_lag_3']
    new_features['cases_lag_3'] = new_features['cases_lag_2']
    new_features['cases_lag_2'] = new_features['cases_lag_1']
    new_features['cases_lag_1'] = prediction # A previsão atual vira o lag da próxima!

    # Atualizar features de tempo
    new_features['week_of_year'] = date.isocalendar().week
    new_features['month'] = date.month
    new_features['year'] = date.year

    # Para clima, podemos usar a média histórica para aquele mês ou repetir o último valor
    # (simplificação)

    current_features = new_features


# 4. Visualizar a previsão final
predictions_series = pd.Series(future_predictions, index=future_dates)

plt.figure(figsize=(15, 8))
plt.plot(df_model['cases'].tail(104), label='Dados Históricos (Últimos 2 anos)')
plt.plot(predictions_series, label='Previsão Futura (Próximas 12 semanas)', color='red', linestyle='--')
plt.title('Previsão Final de Casos de Dengue')
plt.legend()
plt.grid(True)
plt.show()

"""**Etapa 11 - Resultado Final**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Necessário para exibir tabelas formatadas no Colab
from IPython.display import display, HTML

# -------------------------------------------------------------------------
# 5.1. Tabela de Comparação de Métricas
# -------------------------------------------------------------------------

# NOTA: O código assume que as métricas (mae_prophet, rmse_prophet, mae_xgb, rmse_xgb)
# já foram calculadas na Etapa 8/10 do seu notebook.

print("## 5.1. Comparação de Métricas de Avaliação\n")

# Criar o DataFrame de métricas
metrics_data = {
    'Modelo': ['Prophet', 'XGBoost'],
    'MAE (Mean Absolute Error)': [mae_prophet, mae_xgb],
    'RMSE (Root Mean Squared Error)': [rmse_prophet, rmse_xgb]
}
df_metrics = pd.DataFrame(metrics_data)

# Formatar a coluna de métricas para duas casas decimais
format_mapping = {
    'MAE (Mean Absolute Error)': '{:,.2f}'.format,
    'RMSE (Root Mean Squared Error)': '{:,.2f}'.format
}

# Exibir a tabela com formatação
display(HTML("<h3>Tabela de Comparação de Métricas</h3>"))
df_metrics_styled = df_metrics.style.format(format_mapping).hide(axis='index')
display(df_metrics_styled)


# -------------------------------------------------------------------------
# 5.2. Importância das Features (XGBoost)
# -------------------------------------------------------------------------

print("\n\n## 5.2. Importância das Features (XGBoost)\n")

# NOTA: O código assume que 'xgb_reg' (o modelo XGBoost base/vencedor) e 'features' (lista de colunas X)
# estão definidos e treinados.

try:
    # 1. Extrair a importância das features
    feature_importances = pd.Series(
        xgb_reg.feature_importances_,
        index=features
    ).sort_values(ascending=False)

    # 2. Criar a Tabela de Importância
    df_importance = feature_importances.reset_index()
    df_importance.columns = ['Feature', 'Importância (Gain)']

    # 3. Exibir a tabela formatada
    display(HTML("<h3>Tabela de Importância de Features (XGBoost)</h3>"))
    format_mapping_imp = {
        'Importância (Gain)': '{:,.4f}'.format
    }
    df_importance_styled = df_importance.style.format(format_mapping_imp).hide(axis='index')
    display(df_importance_styled)

    # 4. Plotar o gráfico de barras
    plt.figure(figsize=(10, 6))
    feature_importances.sort_values().plot(kind='barh', title='Importância das Features - XGBoost (Gain)')
    plt.xlabel('Importância (Gain)')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()

except NameError:
    print("AVISO: Certifique-se de que 'xgb_reg' (o modelo XGBoost treinado) e 'features' (lista de colunas) estão definidos e executados antes de rodar esta seção.")


# -------------------------------------------------------------------------
# 5.3. Previsão Final (Forecast)
# -------------------------------------------------------------------------

print("\n\n## 5.3. Previsão Final (Forecast)\n")

try:
    # 1. Definir o período histórico a ser plotado (últimas 104 semanas)
    df_history_plot = df_model['cases'].tail(104)

    # 2. Corrigir o TypeError: Garantir que 'future_predictions' é uma Series, não uma lista.
    # O código assume que 'future_predictions' contém as previsões e 'future_dates' (do Etapa 10) o índice.
    # Se 'future_predictions' ainda for uma lista, esta linha irá consertar o tipo:
    if isinstance(future_predictions, list) and 'future_dates' in locals():
        future_predictions = pd.Series(future_predictions, index=future_dates)

    # 3. Concatenar o histórico e a previsão (A linha que estava dando erro)
    df_final_plot = pd.concat([df_history_plot, future_predictions])

    # 4. Plotar o gráfico final
    plt.figure(figsize=(15, 7))

    # Plot histórico (Dados reais)
    plt.plot(df_history_plot.index, df_history_plot.values, label='Dados Históricos (Últimos 2 anos)', color='blue', linewidth=2)

    # Plot previsão futura (apenas o período de forecast)
    plt.plot(future_predictions.index, future_predictions.values, label='Previsão Futura (Próximas 12 semanas)', color='red', linestyle='--', linewidth=2, marker='o', markersize=4)

    plt.title('Previsão Final de Casos de Dengue (XGBoost Otimizado)')
    plt.xlabel('Data')
    plt.ylabel('Casos de Dengue')
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.6)
    plt.tight_layout()
    plt.show()

except NameError as e:
    print(f"AVISO: A Etapa 10 (Treinamento Final) precisa ser executada primeiro. Erro: {e}")
    print("Execute o código da Etapa 10 para definir as variáveis 'df_model', 'future_predictions' e 'future_dates'.")